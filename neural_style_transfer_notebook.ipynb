{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer with Ray and PyTorch\n",
    "\n",
    "This notebook demonstrates distributed neural style transfer using PyTorch and Ray for batch processing of images.\n",
    "\n",
    "## Overview\n",
    "- Uses VGG19 features for style and content representation\n",
    "- Implements Gram matrices for style loss computation\n",
    "- Distributed batch processing with Ray\n",
    "- GPU acceleration support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import ray\n",
    "from ray import data\n",
    "import numpy as np\n",
    "import io\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Ray version: {ray.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ray\n",
    "if not ray.is_initialized():\n",
    "    ray.init()\n",
    "    \n",
    "print(f\"Ray cluster resources: {ray.cluster_resources()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Components\n",
    "\n",
    "### VGG Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGFeatures(nn.Module):\n",
    "    \"\"\"Extract features from VGG19 for style transfer\"\"\"\n",
    "    def __init__(self):\n",
    "        super(VGGFeatures, self).__init__()\n",
    "        # Load pre-trained VGG19\n",
    "        vgg = models.vgg19(pretrained=True).features\n",
    "        self.layers = nn.ModuleList(vgg[:29])  # Up to conv4_4\n",
    "        \n",
    "        # Freeze parameters\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            features.append(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gram Matrix for Style Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GramMatrix(nn.Module):\n",
    "    \"\"\"Compute Gram matrix for style representation\"\"\"\n",
    "    def forward(self, x):\n",
    "        batch, channels, height, width = x.size()\n",
    "        features = x.view(batch * channels, height * width)\n",
    "        gram = torch.mm(features, features.t())\n",
    "        return gram.div(batch * channels * height * width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Style Transfer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransferModel(nn.Module):\n",
    "    \"\"\"Neural Style Transfer model\"\"\"\n",
    "    def __init__(self):\n",
    "        super(StyleTransferModel, self).__init__()\n",
    "        self.vgg = VGGFeatures()\n",
    "        self.gram = GramMatrix()\n",
    "        \n",
    "        # Style layers (conv layers where we compute style loss)\n",
    "        self.style_layers = [0, 5, 10, 19, 28]  # conv1_1, conv2_1, conv3_1, conv4_1, conv5_1\n",
    "        # Content layer\n",
    "        self.content_layer = 21  # conv4_2\n",
    "        \n",
    "    def get_style_features(self, style_image):\n",
    "        \"\"\"Extract style features from style image\"\"\"\n",
    "        features = self.vgg(style_image)\n",
    "        style_features = []\n",
    "        for i in self.style_layers:\n",
    "            style_features.append(self.gram(features[i]))\n",
    "        return style_features\n",
    "    \n",
    "    def get_content_features(self, content_image):\n",
    "        \"\"\"Extract content features from content image\"\"\"\n",
    "        features = self.vgg(content_image)\n",
    "        return features[self.content_layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, size=512):\n",
    "    \"\"\"Preprocess image for neural style transfer\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    if isinstance(image_path, str):\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    else:\n",
    "        # Assume it's already a PIL Image or bytes\n",
    "        if isinstance(image_path, bytes):\n",
    "            image = Image.open(io.BytesIO(image_path)).convert('RGB')\n",
    "        else:\n",
    "            image = image_path.convert('RGB')\n",
    "    \n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "def deprocess_image(tensor):\n",
    "    \"\"\"Convert tensor back to PIL Image\"\"\"\n",
    "    # Denormalize\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    tensor = tensor * std + mean\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "    \n",
    "    # Convert to PIL\n",
    "    tensor = tensor.squeeze(0)\n",
    "    transform = transforms.ToPILImage()\n",
    "    return transform(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Transfer Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_transfer_step(target, content_features, style_features, model, \n",
    "                       content_weight=1, style_weight=1000, iterations=300, verbose=True):\n",
    "    \"\"\"Perform style transfer optimization\"\"\"\n",
    "    target = target.clone().requires_grad_(True)\n",
    "    optimizer = torch.optim.LBFGS([target])\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get features from target\n",
    "        target_features = model.vgg(target)\n",
    "        \n",
    "        # Content loss\n",
    "        content_loss = F.mse_loss(target_features[model.content_layer], content_features)\n",
    "        \n",
    "        # Style loss\n",
    "        style_loss = 0\n",
    "        for i, style_layer_idx in enumerate(model.style_layers):\n",
    "            target_gram = model.gram(target_features[style_layer_idx])\n",
    "            style_loss += F.mse_loss(target_gram, style_features[i])\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "        total_loss.backward()\n",
    "        \n",
    "        losses.append(total_loss.item())\n",
    "        return total_loss\n",
    "    \n",
    "    # Optimization loop\n",
    "    for i in range(iterations):\n",
    "        optimizer.step(closure)\n",
    "        if verbose and i % 50 == 0:\n",
    "            print(f\"Iteration {i}, Loss: {losses[-1]:.4f}\")\n",
    "    \n",
    "    return target, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Predictor for Distributed Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransferPredictor:\n",
    "    \"\"\"Ray predictor class for style transfer\"\"\"\n",
    "    def __init__(self, style_image_path):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = StyleTransferModel().to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Load and preprocess style image\n",
    "        self.style_image = preprocess_image(style_image_path).to(self.device)\n",
    "        self.style_features = self.model.get_style_features(self.style_image)\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"Process a batch of content images\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for item in batch[\"image\"]:\n",
    "            try:\n",
    "                # Preprocess content image\n",
    "                if isinstance(item, str):\n",
    "                    content_image = preprocess_image(item).to(self.device)\n",
    "                else:\n",
    "                    # Handle base64 encoded images or raw bytes\n",
    "                    if isinstance(item, str) and item.startswith('data:image'):\n",
    "                        # Base64 encoded image\n",
    "                        image_data = base64.b64decode(item.split(',')[1])\n",
    "                        content_image = preprocess_image(image_data).to(self.device)\n",
    "                    else:\n",
    "                        content_image = preprocess_image(item).to(self.device)\n",
    "                \n",
    "                # Extract content features\n",
    "                with torch.no_grad():\n",
    "                    content_features = self.model.get_content_features(content_image)\n",
    "                \n",
    "                # Initialize target as content image\n",
    "                target = content_image.clone()\n",
    "                \n",
    "                # Perform style transfer\n",
    "                with torch.enable_grad():\n",
    "                    stylized, losses = style_transfer_step(\n",
    "                        target, content_features, self.style_features, self.model,\n",
    "                        content_weight=1, style_weight=1000, iterations=100, verbose=False\n",
    "                    )\n",
    "                \n",
    "                # Convert back to PIL and then to base64\n",
    "                stylized_pil = deprocess_image(stylized.cpu())\n",
    "                \n",
    "                # Convert to base64 for storage/transmission\n",
    "                buffer = io.BytesIO()\n",
    "                stylized_pil.save(buffer, format='PNG')\n",
    "                img_str = base64.b64encode(buffer.getvalue()).decode()\n",
    "                \n",
    "                results.append({\n",
    "                    \"stylized_image\": f\"data:image/png;base64,{img_str}\",\n",
    "                    \"status\": \"success\"\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"stylized_image\": None,\n",
    "                    \"status\": f\"error: {str(e)}\"\n",
    "                })\n",
    "        \n",
    "        return {\"stylized_image\": [r[\"stylized_image\"] for r in results],\n",
    "                \"status\": [r[\"status\"] for r in results]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Image Style Transfer (Interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_style_transfer(content_path, style_path, iterations=300, content_weight=1, style_weight=1000):\n",
    "    \"\"\"Perform style transfer on a single image with visualization\"\"\"\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model\n",
    "    model = StyleTransferModel().to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess images\n",
    "    content_image = preprocess_image(content_path).to(device)\n",
    "    style_image = preprocess_image(style_path).to(device)\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        content_features = model.get_content_features(content_image)\n",
    "        style_features = model.get_style_features(style_image)\n",
    "    \n",
    "    # Initialize target image\n",
    "    target = content_image.clone()\n",
    "    \n",
    "    # Perform style transfer\n",
    "    print(\"Starting style transfer...\")\n",
    "    stylized, losses = style_transfer_step(\n",
    "        target, content_features, style_features, model,\n",
    "        content_weight=content_weight, style_weight=style_weight, \n",
    "        iterations=iterations, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Convert images for display\n",
    "    content_pil = deprocess_image(content_image.cpu())\n",
    "    style_pil = deprocess_image(style_image.cpu())\n",
    "    stylized_pil = deprocess_image(stylized.cpu())\n",
    "    \n",
    "    # Display results\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(content_pil)\n",
    "    axes[0].set_title('Content Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(style_pil)\n",
    "    axes[1].set_title('Style Image')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(stylized_pil)\n",
    "    axes[2].set_title('Stylized Result')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(losses)\n",
    "    plt.title('Loss During Optimization')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return stylized_pil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing with Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_style_transfer(content_image_paths, style_image_path, output_dir=\"./outputs\"):\n",
    "    \"\"\"Run style transfer on multiple images using Ray\"\"\"\n",
    "    \n",
    "    print(\"Creating dataset...\")\n",
    "    # Create Ray dataset\n",
    "    dataset = ray.data.from_items([{\"image\": path} for path in content_image_paths])\n",
    "    \n",
    "    print(\"Setting up style transfer predictor...\")\n",
    "    # Create predictor with the style image\n",
    "    predictor = StyleTransferPredictor(style_image_path)\n",
    "    \n",
    "    print(\"Running batch inference...\")\n",
    "    # Apply style transfer using Ray Data\n",
    "    results = dataset.map_batches(\n",
    "        predictor,\n",
    "        batch_size=2,  # Process 2 images at a time\n",
    "        num_gpus=1 if torch.cuda.is_available() else 0,\n",
    "        num_cpus=2\n",
    "    )\n",
    "    \n",
    "    print(\"Collecting results...\")\n",
    "    # Collect and save results\n",
    "    output_data = results.take_all()\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    saved_images = []\n",
    "    for i, result in enumerate(output_data):\n",
    "        if result[\"status\"] == \"success\" and result[\"stylized_image\"]:\n",
    "            # Decode base64 and save\n",
    "            img_data = base64.b64decode(result[\"stylized_image\"].split(',')[1])\n",
    "            output_path = os.path.join(output_dir, f\"stylized_image_{i}.png\")\n",
    "            with open(output_path, 'wb') as f:\n",
    "                f.write(img_data)\n",
    "            print(f\"Saved stylized image: {output_path}\")\n",
    "            saved_images.append(output_path)\n",
    "        else:\n",
    "            print(f\"Failed to process image {i}: {result['status']}\")\n",
    "    \n",
    "    return saved_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "### Single Image Style Transfer\n",
    "\n",
    "Replace the paths below with your actual image paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example paths - replace with your actual image paths\n",
    "content_path = \"assets/content1.jpg\"  # Your content image\n",
    "style_path = \"assets/starry_night.jpg\"  # Your style image (e.g., Van Gogh's Starry Night)\n",
    "\n",
    "# Check if files exist\n",
    "if os.path.exists(content_path) and os.path.exists(style_path):\n",
    "    print(\"Running single image style transfer...\")\n",
    "    result = single_style_transfer(content_path, style_path, iterations=200)\n",
    "    \n",
    "    # Save result\n",
    "    result.save(\"single_style_result.png\")\n",
    "    print(\"Result saved as 'single_style_result.png'\")\n",
    "else:\n",
    "    print(f\"Images not found. Please add:\")\n",
    "    print(f\"- Content image: {content_path}\")\n",
    "    print(f\"- Style image: {style_path}\")\n",
    "    print(\"To the assets/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example batch processing\n",
    "content_images = [\n",
    "    \"assets/content1.jpg\",\n",
    "    \"assets/content2.jpg\",\n",
    "    \"assets/content3.jpg\"\n",
    "]\n",
    "\n",
    "style_image = \"assets/starry_night.jpg\"\n",
    "\n",
    "# Check if files exist\n",
    "existing_images = [img for img in content_images if os.path.exists(img)]\n",
    "\n",
    "if existing_images and os.path.exists(style_image):\n",
    "    print(f\"Running batch style transfer on {len(existing_images)} images...\")\n",
    "    \n",
    "    saved_images = run_batch_style_transfer(\n",
    "        content_image_paths=existing_images,\n",
    "        style_image_path=style_image,\n",
    "        output_dir=\"./batch_outputs\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nBatch processing complete! Saved {len(saved_images)} images.\")\n",
    "    \n",
    "    # Display results if any\n",
    "    if saved_images:\n",
    "        fig, axes = plt.subplots(1, min(len(saved_images), 4), figsize=(16, 4))\n",
    "        if len(saved_images) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, img_path in enumerate(saved_images[:4]):\n",
    "            img = Image.open(img_path)\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f'Result {i+1}')\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Please add sample images to the assets/ directory:\")\n",
    "    print(\"- Content images (content1.jpg, content2.jpg, etc.)\")\n",
    "    print(\"- Style image (starry_night.jpg or any style image)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown Ray when done\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "    print(\"Ray shutdown complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "1. **Image Requirements**: Place your images in the `assets/` directory\n",
    "2. **GPU Support**: The notebook will automatically use CUDA if available\n",
    "3. **Memory**: Style transfer can be memory-intensive. Reduce image size if needed\n",
    "4. **Parameters**: Experiment with `content_weight` and `style_weight` for different effects\n",
    "5. **Iterations**: More iterations generally give better results but take longer\n",
    "\n",
    "### Parameter Guidelines:\n",
    "- **content_weight**: Controls content preservation (default: 1)\n",
    "- **style_weight**: Controls style transfer strength (default: 1000)\n",
    "- **iterations**: Number of optimization steps (default: 300)\n",
    "\n",
    "### Troubleshooting:\n",
    "- If you get memory errors, try reducing the image size in `preprocess_image()`\n",
    "- For faster results, reduce the number of iterations\n",
    "- Make sure your images are in RGB format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}